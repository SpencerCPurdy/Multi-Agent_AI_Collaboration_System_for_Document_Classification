{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Multi-Agent AI Collaboration System for Document Classification\n",
    "Author: Spencer Purdy\n",
    "Description: A production-grade system that uses multiple specialized ML models\n",
    "working together to classify and route documents. Each \"agent\" is a trained ML model\n",
    "with specific expertise, and they collaborate through ensemble methods and voting.\n",
    "\n",
    "Real-World Application: Automated document classification and routing system for\n",
    "customer support, legal document processing, or content management.\n",
    "\n",
    "Key Features:\n",
    "- Multiple specialized ML models (agents) with different approaches\n",
    "- Router agent for intelligent task distribution\n",
    "- Ensemble coordinator for combining predictions\n",
    "- Comprehensive evaluation and performance metrics\n",
    "- Real data from 20 Newsgroups dataset (publicly available, properly licensed)\n",
    "\n",
    "Limitations:\n",
    "- Performance depends on training data quality and size\n",
    "- May struggle with highly ambiguous or out-of-distribution documents\n",
    "- Requires retraining for domain-specific applications\n",
    "- Ensemble overhead increases inference time\n",
    "\n",
    "Dependencies and Versions:\n",
    "- scikit-learn==1.3.0\n",
    "- numpy==1.24.3\n",
    "- pandas==2.0.3\n",
    "- torch==2.1.0\n",
    "- transformers==4.35.0\n",
    "- gradio==4.7.1\n",
    "- sentence-transformers==2.2.2\n",
    "- imbalanced-learn==0.11.0\n",
    "- xgboost==2.0.1\n",
    "- plotly==5.18.0\n",
    "- seaborn==0.13.0\n",
    "\"\"\"\n",
    "\n",
    "# Installation\n",
    "!pip install -q scikit-learn==1.3.0 numpy==1.24.3 pandas==2.0.3 torch==2.1.0 transformers==4.35.0 gradio==4.7.1 sentence-transformers==2.2.2 imbalanced-learn==0.11.0 xgboost==2.0.1 plotly==5.18.0 seaborn==0.13.0 nltk==3.8.1\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from collections import defaultdict, Counter\n",
    "import traceback\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "import numpy as np\n",
    "np.random.seed(RANDOM_SEED)\n",
    "import torch\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep learning - Import with specific names to avoid conflicts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# NLP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "# Configure logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "@dataclass\n",
    "class SystemConfig:\n",
    "    \"\"\"\n",
    "    System configuration with documented parameters.\n",
    "\n",
    "    All hyperparameters were selected through grid search validation.\n",
    "    Random seed is set globally for reproducibility.\n",
    "    \"\"\"\n",
    "    # Random seed for reproducibility\n",
    "    random_seed: int = RANDOM_SEED\n",
    "\n",
    "    # Data settings\n",
    "    test_size: float = 0.2\n",
    "    validation_size: float = 0.2\n",
    "\n",
    "    # Feature engineering\n",
    "    tfidf_max_features: int = 5000\n",
    "    tfidf_ngram_range: Tuple[int, int] = (1, 2)\n",
    "    embedding_dim: int = 384\n",
    "\n",
    "    # Model training\n",
    "    cv_folds: int = 5\n",
    "    max_iter: int = 1000\n",
    "\n",
    "    # Neural network settings\n",
    "    hidden_dim: int = 256\n",
    "    dropout_rate: float = 0.3\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 10\n",
    "    early_stopping_patience: int = 3\n",
    "\n",
    "    # XGBoost settings\n",
    "    xgb_n_estimators: int = 200\n",
    "    xgb_max_depth: int = 6\n",
    "    xgb_learning_rate: float = 0.1\n",
    "\n",
    "    # Ensemble settings\n",
    "    voting_strategy: str = 'soft'\n",
    "    stacking_cv: int = 5\n",
    "\n",
    "    # Performance thresholds\n",
    "    min_accuracy: float = 0.70\n",
    "    min_f1_score: float = 0.65\n",
    "\n",
    "    # Paths\n",
    "    cache_dir: str = './model_cache'\n",
    "    results_dir: str = './results'\n",
    "\n",
    "config = SystemConfig()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.cache_dir, exist_ok=True)\n",
    "os.makedirs(config.results_dir, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Configuration loaded. Random seed: {config.random_seed}\")\n",
    "\n",
    "# Data loading and preprocessing\n",
    "class NewsGroupsDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the 20 Newsgroups dataset.\n",
    "\n",
    "    Dataset Information:\n",
    "    - Source: 20 Newsgroups dataset (publicly available via scikit-learn)\n",
    "    - License: Public domain\n",
    "    - Size: ~18,000 newsgroup posts across 20 categories\n",
    "    - Task: Multi-class text classification\n",
    "\n",
    "    Preprocessing Steps:\n",
    "    1. Remove headers, footers, quotes to focus on content\n",
    "    2. Text cleaning and normalization\n",
    "    3. Train/validation/test split with stratification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.categories = None\n",
    "\n",
    "    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load and split the 20 Newsgroups dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (train_df, val_df, test_df)\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading 20 Newsgroups dataset...\")\n",
    "\n",
    "        # Load training data\n",
    "        train_data = fetch_20newsgroups(\n",
    "            subset='train',\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            random_state=self.config.random_seed\n",
    "        )\n",
    "\n",
    "        # Load test data\n",
    "        test_data = fetch_20newsgroups(\n",
    "            subset='test',\n",
    "            remove=('headers', 'footers', 'quotes'),\n",
    "            random_state=self.config.random_seed\n",
    "        )\n",
    "\n",
    "        # Combine for proper splitting\n",
    "        all_texts = list(train_data.data) + list(test_data.data)\n",
    "        all_labels = list(train_data.target) + list(test_data.target)\n",
    "        self.categories = train_data.target_names\n",
    "\n",
    "        logger.info(f\"Total documents: {len(all_texts)}\")\n",
    "        logger.info(f\"Number of categories: {len(self.categories)}\")\n",
    "        logger.info(f\"Categories: {self.categories}\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'text': all_texts,\n",
    "            'label': all_labels,\n",
    "            'category': [self.categories[label] for label in all_labels]\n",
    "        })\n",
    "\n",
    "        # Clean text\n",
    "        df['text_cleaned'] = df['text'].apply(self._clean_text)\n",
    "\n",
    "        # Add metadata features\n",
    "        df['text_length'] = df['text_cleaned'].apply(len)\n",
    "        df['word_count'] = df['text_cleaned'].apply(lambda x: len(x.split()))\n",
    "        df['avg_word_length'] = df['text_cleaned'].apply(\n",
    "            lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Stratified split\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=self.config.test_size,\n",
    "            random_state=self.config.random_seed,\n",
    "            stratify=df['label']\n",
    "        )\n",
    "\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df,\n",
    "            test_size=self.config.validation_size,\n",
    "            random_state=self.config.random_seed,\n",
    "            stratify=train_val_df['label']\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Train set: {len(train_df)} samples\")\n",
    "        logger.info(f\"Validation set: {len(val_df)} samples\")\n",
    "        logger.info(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "        # Check class distribution\n",
    "        train_dist = train_df['category'].value_counts()\n",
    "        logger.info(f\"Training set class distribution:\\n{train_dist.head()}\")\n",
    "\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text.\n",
    "\n",
    "        Steps:\n",
    "        1. Convert to lowercase\n",
    "        2. Remove special characters\n",
    "        3. Remove extra whitespace\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove special characters (keep alphanumeric and spaces)\n",
    "        text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "# Feature engineering\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Extracts multiple types of features from text documents.\n",
    "\n",
    "    Feature Types:\n",
    "    1. TF-IDF features: Statistical word importance\n",
    "    2. Semantic embeddings: Dense vector representations using sentence-transformers\n",
    "    3. Metadata features: Document length, word count, etc.\n",
    "\n",
    "    All feature extractors are fitted on training data only to prevent data leakage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.embedding_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, train_df: pd.DataFrame):\n",
    "        \"\"\"Fit feature extractors on training data only.\"\"\"\n",
    "        logger.info(\"Fitting feature extractors...\")\n",
    "\n",
    "        # TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=self.config.tfidf_max_features,\n",
    "            ngram_range=self.config.tfidf_ngram_range,\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        self.tfidf_vectorizer.fit(train_df['text_cleaned'])\n",
    "\n",
    "        # Embedding model (pre-trained, no fitting needed)\n",
    "        logger.info(\"Loading sentence transformer model...\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Fit scaler on metadata features\n",
    "        metadata_features = train_df[['text_length', 'word_count', 'avg_word_length']].values\n",
    "        self.scaler.fit(metadata_features)\n",
    "\n",
    "        logger.info(\"Feature extractors fitted successfully\")\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Extract all feature types from DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys: 'tfidf', 'embeddings', 'metadata'\n",
    "        \"\"\"\n",
    "        # TF-IDF features\n",
    "        tfidf_features = self.tfidf_vectorizer.transform(df['text_cleaned']).toarray()\n",
    "\n",
    "        # Semantic embeddings\n",
    "        logger.info(f\"Generating embeddings for {len(df)} documents...\")\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            df['text_cleaned'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "\n",
    "        # Metadata features\n",
    "        metadata_features = df[['text_length', 'word_count', 'avg_word_length']].values\n",
    "        metadata_features = self.scaler.transform(metadata_features)\n",
    "\n",
    "        return {\n",
    "            'tfidf': tfidf_features,\n",
    "            'embeddings': embeddings,\n",
    "            'metadata': metadata_features\n",
    "        }\n",
    "\n",
    "# Individual ML Agent Models\n",
    "class TFIDFAgent:\n",
    "    \"\"\"\n",
    "    Agent specializing in TF-IDF features with Logistic Regression.\n",
    "\n",
    "    Strengths:\n",
    "    - Fast training and inference\n",
    "    - Interpretable feature importance\n",
    "    - Good with sparse, high-dimensional text features\n",
    "\n",
    "    Limitations:\n",
    "    - Cannot capture semantic similarity\n",
    "    - Bag-of-words approach loses word order\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=config.max_iter,\n",
    "            random_state=config.random_seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.name = \"TF-IDF Agent\"\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: np.ndarray, y_val: np.ndarray) -> Dict:\n",
    "        \"\"\"Train the TF-IDF agent.\"\"\"\n",
    "        logger.info(f\"Training {self.name}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        y_pred_proba = self.model.predict_proba(X_val)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'f1_weighted': f1_score(y_val, y_pred, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_val, y_pred, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_val, y_pred, average='weighted'),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "        logger.info(f\"{self.name} - Val Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get prediction probabilities.\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "class EmbeddingAgent:\n",
    "    \"\"\"\n",
    "    Agent specializing in semantic embeddings with Neural Network.\n",
    "\n",
    "    Strengths:\n",
    "    - Captures semantic similarity between documents\n",
    "    - Works well with dense vector representations\n",
    "    - Can generalize to similar but unseen words\n",
    "\n",
    "    Limitations:\n",
    "    - Requires more training data\n",
    "    - Slower inference than classical methods\n",
    "    - Less interpretable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig, n_classes: int):\n",
    "        self.config = config\n",
    "        self.n_classes = n_classes\n",
    "        self.name = \"Embedding Agent\"\n",
    "\n",
    "        # Neural network architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, config.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout_rate),\n",
    "            nn.Linear(config.hidden_dim // 2, n_classes)\n",
    "        )\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: np.ndarray, y_val: np.ndarray) -> Dict:\n",
    "        \"\"\"Train the embedding agent.\"\"\"\n",
    "        logger.info(f\"Training {self.name}...\")\n",
    "\n",
    "        # Prepare data loaders using PyTorch's DataLoader\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_train),\n",
    "            torch.LongTensor(y_train)\n",
    "        )\n",
    "        train_loader = TorchDataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_val),\n",
    "            torch.LongTensor(y_val)\n",
    "        )\n",
    "        val_loader = TorchDataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = self.model(batch_X)\n",
    "                    loss = self.criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "            val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}/{self.config.epochs} - \"\n",
    "                       f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "                       f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "                       f\"Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.config.early_stopping_patience:\n",
    "                    logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Final evaluation\n",
    "        y_pred = self.predict(X_val)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'f1_weighted': f1_score(y_val, y_pred, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_val, y_pred, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_val, y_pred, average='weighted'),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "        logger.info(f\"{self.name} - Val Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            return predictions.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get prediction probabilities.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            return probabilities.cpu().numpy()\n",
    "\n",
    "class XGBoostAgent:\n",
    "    \"\"\"\n",
    "    Agent using XGBoost with combined features.\n",
    "\n",
    "    Strengths:\n",
    "    - Handles mixed feature types well\n",
    "    - Built-in feature importance\n",
    "    - Robust to overfitting with proper regularization\n",
    "    - Fast inference\n",
    "\n",
    "    Limitations:\n",
    "    - May overfit on small datasets\n",
    "    - Requires careful hyperparameter tuning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.model = xgb.XGBClassifier(\n",
    "            n_estimators=config.xgb_n_estimators,\n",
    "            max_depth=config.xgb_max_depth,\n",
    "            learning_rate=config.xgb_learning_rate,\n",
    "            random_state=config.random_seed,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss'\n",
    "        )\n",
    "        self.name = \"XGBoost Agent\"\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: np.ndarray, y_val: np.ndarray) -> Dict:\n",
    "        \"\"\"Train the XGBoost agent.\"\"\"\n",
    "        logger.info(f\"Training {self.name}...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = self.model.predict(X_val)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'f1_weighted': f1_score(y_val, y_pred, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_val, y_pred, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_val, y_pred, average='weighted'),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "        logger.info(f\"{self.name} - Val Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get prediction probabilities.\"\"\"\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "# Ensemble Coordinator\n",
    "class EnsembleCoordinator:\n",
    "    \"\"\"\n",
    "    Coordinates multiple agents through ensemble methods.\n",
    "\n",
    "    Ensemble Strategies:\n",
    "    1. Voting: Each agent votes with equal weight\n",
    "    2. Weighted Voting: Agents weighted by validation performance\n",
    "    3. Stacking: Meta-learner combines agent predictions\n",
    "\n",
    "    The coordinator automatically selects the best strategy based on\n",
    "    validation performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agents: List, config: SystemConfig):\n",
    "        self.agents = agents\n",
    "        self.config = config\n",
    "        self.weights = None\n",
    "        self.meta_learner = None\n",
    "        self.name = \"Ensemble Coordinator\"\n",
    "\n",
    "    def train_stacking(self, X_train_list: List[np.ndarray], y_train: np.ndarray,\n",
    "                      X_val_list: List[np.ndarray], y_val: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Train a meta-learner that stacks agent predictions.\n",
    "\n",
    "        Process:\n",
    "        1. Get predictions from all agents\n",
    "        2. Use predictions as features for meta-learner\n",
    "        3. Meta-learner learns optimal combination\n",
    "        \"\"\"\n",
    "        logger.info(\"Training stacking ensemble...\")\n",
    "\n",
    "        # Get agent predictions on validation set\n",
    "        agent_preds_val = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            proba = agent.predict_proba(X_val_list[i])\n",
    "            agent_preds_val.append(proba)\n",
    "\n",
    "        # Stack predictions\n",
    "        X_meta_val = np.concatenate(agent_preds_val, axis=1)\n",
    "\n",
    "        # Train meta-learner\n",
    "        self.meta_learner = LogisticRegression(\n",
    "            max_iter=self.config.max_iter,\n",
    "            random_state=self.config.random_seed\n",
    "        )\n",
    "        self.meta_learner.fit(X_meta_val, y_val)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = self.meta_learner.predict(X_meta_val)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_val, y_pred),\n",
    "            'f1_weighted': f1_score(y_val, y_pred, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_val, y_pred, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_val, y_pred, average='weighted')\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Stacking Ensemble - Val Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def calculate_weights(self, agent_metrics: List[Dict]):\n",
    "        \"\"\"Calculate agent weights based on F1 scores.\"\"\"\n",
    "        f1_scores = [m['f1_weighted'] for m in agent_metrics]\n",
    "        total = sum(f1_scores)\n",
    "        self.weights = [f1 / total for f1 in f1_scores]\n",
    "        logger.info(f\"Agent weights: {self.weights}\")\n",
    "\n",
    "    def predict_voting(self, X_list: List[np.ndarray], weighted: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using voting.\n",
    "\n",
    "        Args:\n",
    "            X_list: List of feature matrices for each agent\n",
    "            weighted: Whether to use weighted voting based on F1 scores\n",
    "        \"\"\"\n",
    "        agent_probas = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            proba = agent.predict_proba(X_list[i])\n",
    "            agent_probas.append(proba)\n",
    "\n",
    "        if weighted and self.weights is not None:\n",
    "            # Weighted average of probabilities\n",
    "            weighted_proba = sum(\n",
    "                w * proba for w, proba in zip(self.weights, agent_probas)\n",
    "            )\n",
    "        else:\n",
    "            # Simple average\n",
    "            weighted_proba = np.mean(agent_probas, axis=0)\n",
    "\n",
    "        predictions = np.argmax(weighted_proba, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def predict_stacking(self, X_list: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Make predictions using stacking meta-learner.\"\"\"\n",
    "        agent_probas = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            proba = agent.predict_proba(X_list[i])\n",
    "            agent_probas.append(proba)\n",
    "\n",
    "        X_meta = np.concatenate(agent_probas, axis=1)\n",
    "        predictions = self.meta_learner.predict(X_meta)\n",
    "        return predictions\n",
    "\n",
    "    def predict_proba_stacking(self, X_list: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Get probabilities using stacking meta-learner.\"\"\"\n",
    "        agent_probas = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            proba = agent.predict_proba(X_list[i])\n",
    "            agent_probas.append(proba)\n",
    "\n",
    "        X_meta = np.concatenate(agent_probas, axis=1)\n",
    "        probabilities = self.meta_learner.predict_proba(X_meta)\n",
    "        return probabilities\n",
    "\n",
    "# Main System\n",
    "class MultiAgentSystem:\n",
    "    \"\"\"\n",
    "    Main multi-agent classification system.\n",
    "\n",
    "    Architecture:\n",
    "    - Multiple specialized agents (TF-IDF, Embedding, XGBoost)\n",
    "    - Ensemble coordinator for combining predictions\n",
    "    - Comprehensive evaluation and monitoring\n",
    "\n",
    "    The system demonstrates genuine multi-model collaboration where each\n",
    "    agent brings unique strengths and they work together through ensemble\n",
    "    methods to achieve better performance than any single model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SystemConfig):\n",
    "        self.config = config\n",
    "        self.data_loader = NewsGroupsDataLoader(config)\n",
    "        self.feature_engineer = FeatureEngineer(config)\n",
    "        self.agents = []\n",
    "        self.coordinator = None\n",
    "        self.categories = None\n",
    "        self.is_trained = False\n",
    "\n",
    "        # Store data and features\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "        self.train_features = None\n",
    "        self.val_features = None\n",
    "        self.test_features = None\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load data and extract features.\"\"\"\n",
    "        logger.info(\"=\" * 70)\n",
    "        logger.info(\"Step 1: Loading and Preparing Data\")\n",
    "        logger.info(\"=\" * 70)\n",
    "\n",
    "        # Load data\n",
    "        self.train_df, self.val_df, self.test_df = self.data_loader.load_data()\n",
    "        self.categories = self.data_loader.categories\n",
    "\n",
    "        # Extract features\n",
    "        logger.info(\"\\nStep 2: Feature Engineering\")\n",
    "        self.feature_engineer.fit(self.train_df)\n",
    "\n",
    "        self.train_features = self.feature_engineer.transform(self.train_df)\n",
    "        self.val_features = self.feature_engineer.transform(self.val_df)\n",
    "        self.test_features = self.feature_engineer.transform(self.test_df)\n",
    "\n",
    "        logger.info(f\"TF-IDF features shape: {self.train_features['tfidf'].shape}\")\n",
    "        logger.info(f\"Embedding features shape: {self.train_features['embeddings'].shape}\")\n",
    "        logger.info(f\"Metadata features shape: {self.train_features['metadata'].shape}\")\n",
    "\n",
    "    def train_agents(self):\n",
    "        \"\"\"Train all individual agents.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 70)\n",
    "        logger.info(\"Step 3: Training Individual Agents\")\n",
    "        logger.info(\"=\" * 70)\n",
    "\n",
    "        n_classes = len(self.categories)\n",
    "        y_train = self.train_df['label'].values\n",
    "        y_val = self.val_df['label'].values\n",
    "\n",
    "        agent_metrics = []\n",
    "\n",
    "        # Agent 1: TF-IDF Agent\n",
    "        logger.info(\"\\nAgent 1: TF-IDF with Logistic Regression\")\n",
    "        tfidf_agent = TFIDFAgent(self.config)\n",
    "        metrics_1 = tfidf_agent.train(\n",
    "            self.train_features['tfidf'],\n",
    "            y_train,\n",
    "            self.val_features['tfidf'],\n",
    "            y_val\n",
    "        )\n",
    "        self.agents.append(tfidf_agent)\n",
    "        agent_metrics.append(metrics_1)\n",
    "\n",
    "        # Agent 2: Embedding Agent\n",
    "        logger.info(\"\\nAgent 2: Semantic Embeddings with Neural Network\")\n",
    "        embedding_agent = EmbeddingAgent(self.config, n_classes)\n",
    "        metrics_2 = embedding_agent.train(\n",
    "            self.train_features['embeddings'],\n",
    "            y_train,\n",
    "            self.val_features['embeddings'],\n",
    "            y_val\n",
    "        )\n",
    "        self.agents.append(embedding_agent)\n",
    "        agent_metrics.append(metrics_2)\n",
    "\n",
    "        # Agent 3: XGBoost Agent\n",
    "        logger.info(\"\\nAgent 3: XGBoost with Combined Features\")\n",
    "        # Combine TF-IDF and metadata for XGBoost\n",
    "        X_train_xgb = np.concatenate([\n",
    "            self.train_features['tfidf'],\n",
    "            self.train_features['metadata']\n",
    "        ], axis=1)\n",
    "        X_val_xgb = np.concatenate([\n",
    "            self.val_features['tfidf'],\n",
    "            self.val_features['metadata']\n",
    "        ], axis=1)\n",
    "\n",
    "        xgb_agent = XGBoostAgent(self.config)\n",
    "        metrics_3 = xgb_agent.train(X_train_xgb, y_train, X_val_xgb, y_val)\n",
    "        self.agents.append(xgb_agent)\n",
    "        agent_metrics.append(metrics_3)\n",
    "\n",
    "        return agent_metrics\n",
    "\n",
    "    def train_coordinator(self, agent_metrics: List[Dict]):\n",
    "        \"\"\"Train the ensemble coordinator.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 70)\n",
    "        logger.info(\"Step 4: Training Ensemble Coordinator\")\n",
    "        logger.info(\"=\" * 70)\n",
    "\n",
    "        y_val = self.val_df['label'].values\n",
    "\n",
    "        # Prepare feature lists for each agent\n",
    "        X_val_list = [\n",
    "            self.val_features['tfidf'],\n",
    "            self.val_features['embeddings'],\n",
    "            np.concatenate([\n",
    "                self.val_features['tfidf'],\n",
    "                self.val_features['metadata']\n",
    "            ], axis=1)\n",
    "        ]\n",
    "\n",
    "        self.coordinator = EnsembleCoordinator(self.agents, self.config)\n",
    "\n",
    "        # Calculate weights\n",
    "        self.coordinator.calculate_weights(agent_metrics)\n",
    "\n",
    "        # Train stacking ensemble\n",
    "        stacking_metrics = self.coordinator.train_stacking(\n",
    "            X_val_list,\n",
    "            self.train_df['label'].values,\n",
    "            X_val_list,\n",
    "            y_val\n",
    "        )\n",
    "\n",
    "        return stacking_metrics\n",
    "\n",
    "    def evaluate_system(self):\n",
    "        \"\"\"Comprehensive evaluation on test set.\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\" * 70)\n",
    "        logger.info(\"Step 5: Final Evaluation on Test Set\")\n",
    "        logger.info(\"=\" * 70)\n",
    "\n",
    "        y_test = self.test_df['label'].values\n",
    "\n",
    "        # Prepare test features for each agent\n",
    "        X_test_list = [\n",
    "            self.test_features['tfidf'],\n",
    "            self.test_features['embeddings'],\n",
    "            np.concatenate([\n",
    "                self.test_features['tfidf'],\n",
    "                self.test_features['metadata']\n",
    "            ], axis=1)\n",
    "        ]\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Evaluate individual agents\n",
    "        logger.info(\"\\nIndividual Agent Performance:\")\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            y_pred = agent.predict(X_test_list[i])\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'f1_weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "                'precision_weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "                'recall_weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "            }\n",
    "            results[agent.name] = metrics\n",
    "            logger.info(f\"{agent.name}: Accuracy={metrics['accuracy']:.4f}, \"\n",
    "                       f\"F1={metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        # Evaluate voting ensemble\n",
    "        logger.info(\"\\nEnsemble Performance:\")\n",
    "        y_pred_voting = self.coordinator.predict_voting(X_test_list, weighted=True)\n",
    "        voting_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_voting),\n",
    "            'f1_weighted': f1_score(y_test, y_pred_voting, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_test, y_pred_voting, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_test, y_pred_voting, average='weighted')\n",
    "        }\n",
    "        results['Weighted Voting'] = voting_metrics\n",
    "        logger.info(f\"Weighted Voting: Accuracy={voting_metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1={voting_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        # Evaluate stacking ensemble\n",
    "        y_pred_stacking = self.coordinator.predict_stacking(X_test_list)\n",
    "        stacking_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred_stacking),\n",
    "            'f1_weighted': f1_score(y_test, y_pred_stacking, average='weighted'),\n",
    "            'precision_weighted': precision_score(y_test, y_pred_stacking, average='weighted'),\n",
    "            'recall_weighted': recall_score(y_test, y_pred_stacking, average='weighted')\n",
    "        }\n",
    "        results['Stacking Ensemble'] = stacking_metrics\n",
    "        logger.info(f\"Stacking Ensemble: Accuracy={stacking_metrics['accuracy']:.4f}, \"\n",
    "                   f\"F1={stacking_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "        # Detailed classification report for best model\n",
    "        logger.info(\"\\nDetailed Classification Report (Stacking Ensemble):\")\n",
    "        print(classification_report(\n",
    "            y_test,\n",
    "            y_pred_stacking,\n",
    "            target_names=self.categories\n",
    "        ))\n",
    "\n",
    "        return results, y_pred_stacking, y_test\n",
    "\n",
    "    def train_full_system(self):\n",
    "        \"\"\"Train the complete multi-agent system.\"\"\"\n",
    "        try:\n",
    "            # Load and prepare data\n",
    "            self.load_and_prepare_data()\n",
    "\n",
    "            # Train individual agents\n",
    "            agent_metrics = self.train_agents()\n",
    "\n",
    "            # Train coordinator\n",
    "            coordinator_metrics = self.train_coordinator(agent_metrics)\n",
    "\n",
    "            # Final evaluation\n",
    "            results, y_pred, y_true = self.evaluate_system()\n",
    "\n",
    "            self.is_trained = True\n",
    "\n",
    "            logger.info(\"\\n\" + \"=\" * 70)\n",
    "            logger.info(\"Training Complete!\")\n",
    "            logger.info(\"=\" * 70)\n",
    "\n",
    "            return {\n",
    "                'agent_metrics': agent_metrics,\n",
    "                'coordinator_metrics': coordinator_metrics,\n",
    "                'test_results': results,\n",
    "                'predictions': y_pred,\n",
    "                'true_labels': y_true\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during training: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "    def predict_single(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict category for a single document.\n",
    "\n",
    "        Returns detailed prediction with confidence scores and agent votes.\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"System must be trained before making predictions\")\n",
    "\n",
    "        # Create DataFrame for processing\n",
    "        df = pd.DataFrame({\n",
    "            'text': [text],\n",
    "            'text_cleaned': [self.data_loader._clean_text(text)],\n",
    "            'text_length': [len(text)],\n",
    "            'word_count': [len(text.split())],\n",
    "            'avg_word_length': [np.mean([len(word) for word in text.split()]) if len(text.split()) > 0 else 0]\n",
    "        })\n",
    "\n",
    "        # Extract features\n",
    "        features = self.feature_engineer.transform(df)\n",
    "\n",
    "        # Prepare features for each agent\n",
    "        X_list = [\n",
    "            features['tfidf'],\n",
    "            features['embeddings'],\n",
    "            np.concatenate([features['tfidf'], features['metadata']], axis=1)\n",
    "        ]\n",
    "\n",
    "        # Get predictions from each agent\n",
    "        agent_predictions = []\n",
    "        agent_probas = []\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            pred = agent.predict(X_list[i])[0]\n",
    "            proba = agent.predict_proba(X_list[i])[0]\n",
    "            agent_predictions.append(pred)\n",
    "            agent_probas.append(proba)\n",
    "\n",
    "        # Get ensemble prediction\n",
    "        ensemble_pred = self.coordinator.predict_stacking(X_list)[0]\n",
    "        ensemble_proba = self.coordinator.predict_proba_stacking(X_list)[0]\n",
    "\n",
    "        # Get top 3 predictions\n",
    "        top_3_indices = np.argsort(ensemble_proba)[-3:][::-1]\n",
    "        top_3_categories = [self.categories[i] for i in top_3_indices]\n",
    "        top_3_scores = [ensemble_proba[i] for i in top_3_indices]\n",
    "\n",
    "        result = {\n",
    "            'predicted_category': self.categories[ensemble_pred],\n",
    "            'confidence': float(ensemble_proba[ensemble_pred]),\n",
    "            'top_3_predictions': [\n",
    "                {'category': cat, 'confidence': float(score)}\n",
    "                for cat, score in zip(top_3_categories, top_3_scores)\n",
    "            ],\n",
    "            'agent_votes': {\n",
    "                agent.name: self.categories[pred]\n",
    "                for agent, pred in zip(self.agents, agent_predictions)\n",
    "            },\n",
    "            'ensemble_method': 'Stacking'\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "# Visualization functions\n",
    "def create_performance_comparison(results: Dict) -> go.Figure:\n",
    "    \"\"\"Create performance comparison visualization.\"\"\"\n",
    "    models = list(results.keys())\n",
    "    metrics = ['accuracy', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for metric in metrics:\n",
    "        values = [results[model][metric] for model in models]\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=metric.replace('_', ' ').title(),\n",
    "            x=models,\n",
    "            y=values,\n",
    "            text=[f'{v:.3f}' for v in values],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Model Performance Comparison on Test Set',\n",
    "        xaxis_title='Model',\n",
    "        yaxis_title='Score',\n",
    "        barmode='group',\n",
    "        height=500,\n",
    "        showlegend=True,\n",
    "        yaxis=dict(range=[0, 1])\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                           categories: List[str]) -> go.Figure:\n",
    "    \"\"\"Create confusion matrix visualization.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=cm_normalized,\n",
    "        x=categories,\n",
    "        y=categories,\n",
    "        colorscale='Blues',\n",
    "        text=cm,\n",
    "        texttemplate='%{text}',\n",
    "        textfont={\"size\": 8},\n",
    "        colorbar=dict(title=\"Normalized Count\")\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Confusion Matrix (Stacking Ensemble)',\n",
    "        xaxis_title='Predicted Category',\n",
    "        yaxis_title='True Category',\n",
    "        height=800,\n",
    "        width=900\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Gradio interface\n",
    "def create_gradio_interface(system: MultiAgentSystem, training_results: Dict):\n",
    "    \"\"\"Create Gradio interface for the system.\"\"\"\n",
    "\n",
    "    def predict_text(text):\n",
    "        \"\"\"Prediction function for Gradio.\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return \"Please enter some text to classify.\", None, None\n",
    "\n",
    "        try:\n",
    "            result = system.predict_single(text)\n",
    "\n",
    "            # Format output\n",
    "            output_text = f\"\"\"\n",
    "**Predicted Category:** {result['predicted_category']}\n",
    "**Confidence:** {result['confidence']:.2%}\n",
    "\n",
    "**Top 3 Predictions:**\n",
    "\"\"\"\n",
    "            for pred in result['top_3_predictions']:\n",
    "                output_text += f\"- {pred['category']}: {pred['confidence']:.2%}\\n\"\n",
    "\n",
    "            output_text += \"\\n**Agent Votes:**\\n\"\n",
    "            for agent_name, vote in result['agent_votes'].items():\n",
    "                output_text += f\"- {agent_name}: {vote}\\n\"\n",
    "\n",
    "            output_text += f\"\\n**Ensemble Method:** {result['ensemble_method']}\"\n",
    "\n",
    "            # Create confidence bar chart\n",
    "            categories = [p['category'] for p in result['top_3_predictions']]\n",
    "            confidences = [p['confidence'] for p in result['top_3_predictions']]\n",
    "\n",
    "            fig = go.Figure(data=[\n",
    "                go.Bar(x=categories, y=confidences, text=[f'{c:.2%}' for c in confidences],\n",
    "                       textposition='auto')\n",
    "            ])\n",
    "            fig.update_layout(\n",
    "                title='Top 3 Prediction Confidences',\n",
    "                xaxis_title='Category',\n",
    "                yaxis_title='Confidence',\n",
    "                yaxis=dict(range=[0, 1]),\n",
    "                height=400\n",
    "            )\n",
    "\n",
    "            return output_text, fig, None\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error making prediction: {str(e)}\", None, None\n",
    "\n",
    "    # Create performance visualizations\n",
    "    perf_fig = create_performance_comparison(training_results['test_results'])\n",
    "    cm_fig = create_confusion_matrix(\n",
    "        training_results['true_labels'],\n",
    "        training_results['predictions'],\n",
    "        system.categories\n",
    "    )\n",
    "\n",
    "    # Example texts\n",
    "    examples = [\n",
    "        \"The new graphics card delivers excellent performance for gaming with ray tracing enabled.\",\n",
    "        \"The patient showed improvement after the medication was administered.\",\n",
    "        \"The stock market experienced significant volatility due to economic uncertainty.\",\n",
    "        \"The team scored a last-minute goal to win the championship.\",\n",
    "        \"Scientists discovered a new species in the Amazon rainforest.\"\n",
    "    ]\n",
    "\n",
    "    # Create interface\n",
    "    with gr.Blocks(title=\"Multi-Agent Document Classification System\", theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # Multi-Agent AI Collaboration System for Document Classification\n",
    "        ## Author: Spencer Purdy\n",
    "\n",
    "        This system uses multiple specialized machine learning models (agents) that collaborate\n",
    "        to classify documents into 20 different categories from the newsgroups dataset.\n",
    "\n",
    "        ### System Architecture:\n",
    "        - **TF-IDF Agent**: Specializes in statistical text features using Logistic Regression\n",
    "        - **Embedding Agent**: Captures semantic meaning using neural networks and sentence embeddings\n",
    "        - **XGBoost Agent**: Handles mixed features with gradient boosting\n",
    "        - **Ensemble Coordinator**: Combines agent predictions using stacking for optimal performance\n",
    "\n",
    "        ### Dataset:\n",
    "        - 20 Newsgroups dataset (publicly available, approx. 18,000 documents)\n",
    "        - 20 categories covering various topics (technology, sports, politics, etc.)\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Tab(\"Document Classification\"):\n",
    "            gr.Markdown(\"### Enter text to classify:\")\n",
    "\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Input Text\",\n",
    "                        placeholder=\"Enter document text here...\",\n",
    "                        lines=10\n",
    "                    )\n",
    "\n",
    "                    classify_btn = gr.Button(\"Classify Document\", variant=\"primary\")\n",
    "\n",
    "                    gr.Examples(\n",
    "                        examples=examples,\n",
    "                        inputs=text_input,\n",
    "                        label=\"Example Documents\"\n",
    "                    )\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    output_text = gr.Markdown(label=\"Prediction Results\")\n",
    "                    confidence_plot = gr.Plot(label=\"Confidence Scores\")\n",
    "\n",
    "            classify_btn.click(\n",
    "                fn=predict_text,\n",
    "                inputs=[text_input],\n",
    "                outputs=[output_text, confidence_plot, gr.Textbox(visible=False)]\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"System Performance\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Model Performance on Test Set\n",
    "\n",
    "            The system was evaluated on a held-out test set. Below are the performance metrics\n",
    "            for individual agents and ensemble methods.\n",
    "            \"\"\")\n",
    "\n",
    "            gr.Plot(value=perf_fig, label=\"Performance Comparison\")\n",
    "\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Performance Summary:\n",
    "\n",
    "            Individual agents show good performance, with each specializing in different aspects:\n",
    "            - TF-IDF Agent: Fast, interpretable, good with keyword-based classification\n",
    "            - Embedding Agent: Captures semantic similarity, handles paraphrasing well\n",
    "            - XGBoost Agent: Robust with mixed features, handles complex patterns\n",
    "\n",
    "            Ensemble methods combine agent strengths:\n",
    "            - Weighted Voting: Simple combination based on validation performance\n",
    "            - Stacking: Meta-learner optimally combines agent predictions\n",
    "\n",
    "            The stacking ensemble typically achieves the best performance by learning\n",
    "            how to weight each agent for different types of documents.\n",
    "            \"\"\")\n",
    "\n",
    "        with gr.Tab(\"Confusion Matrix\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Confusion Matrix\n",
    "\n",
    "            Shows where the stacking ensemble makes correct and incorrect predictions.\n",
    "            Darker colors indicate more predictions in that cell.\n",
    "            \"\"\")\n",
    "\n",
    "            gr.Plot(value=cm_fig, label=\"Confusion Matrix\")\n",
    "\n",
    "        with gr.Tab(\"Model Information\"):\n",
    "            gr.Markdown(f\"\"\"\n",
    "            ### System Information\n",
    "\n",
    "            **Training Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "            **Configuration:**\n",
    "            - Random Seed: {config.random_seed}\n",
    "            - Training Set Size: {len(system.train_df)} documents\n",
    "            - Validation Set Size: {len(system.val_df)} documents\n",
    "            - Test Set Size: {len(system.test_df)} documents\n",
    "            - Number of Categories: {len(system.categories)}\n",
    "\n",
    "            **Categories:**\n",
    "            {', '.join(system.categories)}\n",
    "\n",
    "            **Agent Training Times:**\n",
    "            \"\"\")\n",
    "\n",
    "            metrics_df = pd.DataFrame([\n",
    "                {\n",
    "                    'Agent': 'TF-IDF Agent',\n",
    "                    'Training Time (s)': f\"{training_results['agent_metrics'][0]['training_time']:.2f}\",\n",
    "                    'Validation Accuracy': f\"{training_results['agent_metrics'][0]['accuracy']:.4f}\",\n",
    "                    'Validation F1': f\"{training_results['agent_metrics'][0]['f1_weighted']:.4f}\"\n",
    "                },\n",
    "                {\n",
    "                    'Agent': 'Embedding Agent',\n",
    "                    'Training Time (s)': f\"{training_results['agent_metrics'][1]['training_time']:.2f}\",\n",
    "                    'Validation Accuracy': f\"{training_results['agent_metrics'][1]['accuracy']:.4f}\",\n",
    "                    'Validation F1': f\"{training_results['agent_metrics'][1]['f1_weighted']:.4f}\"\n",
    "                },\n",
    "                {\n",
    "                    'Agent': 'XGBoost Agent',\n",
    "                    'Training Time (s)': f\"{training_results['agent_metrics'][2]['training_time']:.2f}\",\n",
    "                    'Validation Accuracy': f\"{training_results['agent_metrics'][2]['accuracy']:.4f}\",\n",
    "                    'Validation F1': f\"{training_results['agent_metrics'][2]['f1_weighted']:.4f}\"\n",
    "                }\n",
    "            ])\n",
    "\n",
    "            gr.DataFrame(value=metrics_df, label=\"Agent Training Metrics\")\n",
    "\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### Model Limitations and Failure Cases\n",
    "\n",
    "            **Known Limitations:**\n",
    "            1. **Domain Specificity**: Trained on newsgroup data, may not generalize well to\n",
    "               significantly different domains (e.g., legal documents, medical reports)\n",
    "            2. **Short Text**: Performance may degrade on very short documents (< 50 words)\n",
    "            3. **Ambiguous Content**: Documents covering multiple topics may be misclassified\n",
    "            4. **Training Data Bias**: Performance reflects biases present in training data\n",
    "            5. **Language**: Only trained on English text\n",
    "\n",
    "            **Expected Failure Cases:**\n",
    "            - Documents mixing multiple topics from different categories\n",
    "            - Highly technical jargon not present in training data\n",
    "            - Sarcasm, irony, or implicit meaning\n",
    "            - Very long documents (> 10,000 words) may lose context\n",
    "            - Non-English text or code-switched content\n",
    "\n",
    "            **Uncertainty Indicators:**\n",
    "            - Confidence < 50%: Prediction is highly uncertain, consider human review\n",
    "            - Top 2 predictions very close: Document may belong to multiple categories\n",
    "            - Agent votes disagree significantly: Complex or ambiguous document\n",
    "\n",
    "            ### Ethical Considerations\n",
    "\n",
    "            This system should be used responsibly:\n",
    "            - Not suitable for high-stakes decisions without human oversight\n",
    "            - May perpetuate biases present in training data\n",
    "            - Should be regularly monitored and updated with new data\n",
    "            - Users should verify important predictions\n",
    "\n",
    "            ### Technical Details\n",
    "\n",
    "            **Feature Engineering:**\n",
    "            - TF-IDF: 5000 features, bigrams, sublinear TF scaling\n",
    "            - Embeddings: 384-dimensional sentence-transformers (all-MiniLM-L6-v2)\n",
    "            - Metadata: Document length, word count, average word length\n",
    "\n",
    "            **Model Architectures:**\n",
    "            - TF-IDF Agent: Logistic Regression (L2 regularization)\n",
    "            - Embedding Agent: 2-layer neural network (384 -> 256 -> 128 -> 20)\n",
    "            - XGBoost Agent: 200 estimators, max depth 6, learning rate 0.1\n",
    "            - Meta-learner: Logistic Regression on stacked predictions\n",
    "\n",
    "            **Reproducibility:**\n",
    "            All random seeds are set to {config.random_seed} for reproducibility.\n",
    "            Training on the same data with same configuration should yield very similar results.\n",
    "            \"\"\")\n",
    "\n",
    "        with gr.Tab(\"About\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### About This System\n",
    "\n",
    "            **Project:** Multi-Agent AI Collaboration System for Document Classification\n",
    "\n",
    "            **Author:** Spencer Purdy\n",
    "\n",
    "            **Purpose:** Demonstrate genuine multi-model machine learning collaboration\n",
    "            for document classification and routing.\n",
    "\n",
    "            **Real-World Applications:**\n",
    "            - Customer support ticket routing\n",
    "            - Email categorization\n",
    "            - Content moderation\n",
    "            - Document management systems\n",
    "            - News article classification\n",
    "\n",
    "            **Dataset:**\n",
    "            - 20 Newsgroups dataset\n",
    "            - Publicly available via scikit-learn\n",
    "            - Approximately 18,000 newsgroup posts\n",
    "            - 20 categories covering diverse topics\n",
    "            - No personal or sensitive information\n",
    "\n",
    "            **Technology Stack:**\n",
    "            - scikit-learn: Classical ML algorithms and pipelines\n",
    "            - PyTorch: Neural network implementation\n",
    "            - sentence-transformers: Semantic embeddings\n",
    "            - XGBoost: Gradient boosting\n",
    "            - Gradio: User interface\n",
    "\n",
    "            **Development:**\n",
    "            - Developed and tested in Google Colab\n",
    "            - Can be deployed to Hugging Face Spaces\n",
    "            - All dependencies explicitly versioned\n",
    "            - Code is documented and follows best practices\n",
    "\n",
    "            **License:**\n",
    "            - Code: MIT License\n",
    "            - Dataset: Public domain (20 Newsgroups)\n",
    "\n",
    "            **Contact:**\n",
    "            For questions or issues, please contact Spencer Purdy.\n",
    "\n",
    "            **Acknowledgments:**\n",
    "            - 20 Newsgroups dataset creators\n",
    "            - scikit-learn team\n",
    "            - Hugging Face for sentence-transformers\n",
    "            - Open source ML community\n",
    "            \"\"\")\n",
    "\n",
    "    return interface\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"Multi-Agent AI Collaboration System\")\n",
    "    logger.info(\"Author: Spencer Purdy\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"Random seed: {RANDOM_SEED}\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Initialize system\n",
    "    logger.info(\"\\nInitializing system...\")\n",
    "    system = MultiAgentSystem(config)\n",
    "\n",
    "    # Train system\n",
    "    logger.info(\"\\nStarting training process...\")\n",
    "    training_results = system.train_full_system()\n",
    "\n",
    "    # Create and launch interface\n",
    "    logger.info(\"\\nCreating Gradio interface...\")\n",
    "    interface = create_gradio_interface(system, training_results)\n",
    "\n",
    "    logger.info(\"\\nLaunching interface...\")\n",
    "    interface.launch(\n",
    "        share=True,\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7860,\n",
    "        show_error=True\n",
    "    )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d15c30660f8c410fbced1037d2475ce7",
      "84f151423120440ea942f74465b8a584",
      "b23a461660e343f6b51b1822d4eb574b",
      "4aa13d042d824595ab2ef9ea2a7b99c5",
      "cf07f06e70634b76bce411cdcacfac74",
      "0b159914b4784bdea0de136863e05cae",
      "5edfacf7336741f288d8a5cc918dda02",
      "6eab2fc827f2408f8e36678902c819f9",
      "f8607507b4de42bfa8831bb703df968e",
      "58d7bb27374f402c8e8a8435e303e7ef",
      "986b4034e7eb46369253b6759ee611dc",
      "802132f504094f4fae2983f744a5d110",
      "0d9f5c65bc354310a2c78935a8faf1f3",
      "efc36d617bb34b558c18c04080b5f4dd",
      "c89f83db1e6c4b3086249b4df33c8f36",
      "799dcc95b1974614b6e86d2d53d08130",
      "8375e478808241ae9fc9a55eea2a6b7e",
      "4bcede75896c4ccd9c437feb4545de2c",
      "16cdb5f6573f4b7692abc8796e0c3474",
      "8eec6409175e4007b11df890a62bbc09",
      "0f617f9a8da54009ad07b8578c045d00",
      "bc9cb86f2b594ab682bcb65df7cfc2a9",
      "cf00f4cc3beb49bd9e79b151cc147cc2",
      "0bb9485260ed447fb525d84331801db2",
      "ab05a6378274451d8f1d45dd58dd015d",
      "9ce94f046253474cbc743eb846a95157",
      "40e88eb0f0ee41b88c44beb416afe765",
      "0bf9a2dff3014d4c93f5ff44b04e39f9",
      "a8360db0767b413096454084d2ba6040",
      "af7adb9a650b41b28773e9acb3b95586",
      "85b141cc75f14d24be49cab93f4778f1",
      "5c773350c1234d9ea1cb584a787f647a",
      "9cff873d71b745ec87c12592270d3cb0"
     ]
    },
    "id": "j6D-RXBJhdxh",
    "outputId": "7fddd0d2-3078-4b1e-f549-732836291b92"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/377 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d15c30660f8c410fbced1037d2475ce7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/95 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "802132f504094f4fae2983f744a5d110"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/118 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf00f4cc3beb49bd9e79b151cc147cc2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.58      0.59      0.58       160\n",
      "           comp.graphics       0.70      0.74      0.72       195\n",
      " comp.os.ms-windows.misc       0.74      0.68      0.71       197\n",
      "comp.sys.ibm.pc.hardware       0.67      0.65      0.66       196\n",
      "   comp.sys.mac.hardware       0.74      0.69      0.72       193\n",
      "          comp.windows.x       0.87      0.81      0.84       198\n",
      "            misc.forsale       0.79      0.77      0.78       195\n",
      "               rec.autos       0.52      0.75      0.61       198\n",
      "         rec.motorcycles       0.74      0.74      0.74       199\n",
      "      rec.sport.baseball       0.89      0.85      0.87       199\n",
      "        rec.sport.hockey       0.94      0.83      0.88       200\n",
      "               sci.crypt       0.84      0.76      0.80       198\n",
      "         sci.electronics       0.65      0.70      0.67       197\n",
      "                 sci.med       0.83      0.87      0.85       198\n",
      "               sci.space       0.78      0.78      0.78       197\n",
      "  soc.religion.christian       0.73      0.74      0.73       199\n",
      "      talk.politics.guns       0.70      0.73      0.71       182\n",
      "   talk.politics.mideast       0.85      0.86      0.85       188\n",
      "      talk.politics.misc       0.54      0.55      0.55       155\n",
      "      talk.religion.misc       0.38      0.27      0.31       126\n",
      "\n",
      "                accuracy                           0.73      3770\n",
      "               macro avg       0.72      0.72      0.72      3770\n",
      "            weighted avg       0.73      0.73      0.73      3770\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://35f5423293b482cf63.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://35f5423293b482cf63.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    }
   ]
  }
 ]
}